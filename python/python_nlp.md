# ğŸ“˜ ìì—°ì–´ ì²˜ë¦¬ ë¶„ë¥˜ ëª¨ë¸ì˜ ë°œì „ì‚¬

ìì—°ì–´ ì²˜ë¦¬ëŠ” ì¸ê°„ ì–¸ì–´ë¥¼ ì»´í“¨í„°ê°€ ì´í•´í•  ìˆ˜ ìˆë„ë¡ í•˜ëŠ” ê¸°ìˆ ì…ë‹ˆë‹¤.

---

## ğŸ§­ ê°œìš”

í…ìŠ¤íŠ¸ ë¶„ë¥˜ëŠ” ì´ˆê¸° ê·œì¹™ ê¸°ë°˜ì—ì„œ ì‹œì‘í•˜ì—¬ ë”¥ëŸ¬ë‹, Transformerë¡œ ë°œì „í•´ì™”ìŠµë‹ˆë‹¤.


## ğŸ§© 1. ì›í•« ì¸ì½”ë”© / BoW / TF-IDF

### ğŸ“– ì´ë¡ 
- **ì›í•« ì¸ì½”ë”©**: ë‹¨ì–´ í•˜ë‚˜ë¥¼ ê³ ìœ í•œ ë²¡í„°ë¡œ í‘œí˜„ (1 ì™¸ ë‚˜ë¨¸ì§€ëŠ” 0)
- **BoW (Bag of Words)**: ë‹¨ì–´ì˜ ì¡´ì¬ ì—¬ë¶€ ë˜ëŠ” ë“±ì¥ ë¹ˆë„ ë²¡í„°í™”
- **TF-IDF**: ë‹¨ì–´ ë¹ˆë„ì— ì—­ë¬¸ì„œ ë¹ˆë„ë¥¼ ê³±í•˜ì—¬ ì¤‘ìš” ë‹¨ì–´ ê°•ì¡°

> âŒ ë‹¨ì : ì˜ë¯¸ ì •ë³´ ì†ì‹¤, ì°¨ì› ìˆ˜ê°€ ë§ìŒ


## ğŸ§¬ 2. Word2Vec / GloVe ì„ë² ë”©

### ğŸ“– ì´ë¡ 
- **Word2Vec** (Mikolov, 2013): ì˜ë¯¸ ê¸°ë°˜ ì„ë² ë”©
- CBOW / Skip-gram êµ¬ì¡°
- ìœ ì‚¬í•œ ì˜ë¯¸ â†’ ìœ ì‚¬í•œ ë²¡í„°

> âœ… ì¥ì : ì˜ë¯¸ ë°˜ì˜ ê°€ëŠ¥  
> âŒ ë‹¨ì : ë¬¸ì¥ ì „ì²´ í‘œí˜„ ë¶ˆê°€


## ğŸ§  3. LSTM ê¸°ë°˜ ë¶„ë¥˜ê¸° (ê°„ë‹¨ êµ¬ì¡°)

> - ë‹¨ì–´ ìˆœì„œ ë°˜ì˜  
> - ë¬¸ë§¥ ì´í•´ ê°€ëŠ¥  
> - ê¸´ ë¬¸ì¥ì—ì„œ gradient ì†Œì‹¤ ë¬¸ì œ ìˆìŒ

(ì—¬ê¸°ì„  ìƒëµ ë˜ëŠ” PyTorchë¡œ êµ¬ì„± ê°€ëŠ¥)


## ğŸ”® 4. Transformer ê¸°ë°˜ ë¶„ë¥˜ (BERT)

### ğŸ“– ì´ë¡ 
- BERT: ì–‘ë°©í–¥ ë¬¸ë§¥ ê³ ë ¤
- ì‚¬ì „í•™ìŠµ + íŒŒì¸íŠœë‹ êµ¬ì¡°
- ë‹¤ì–‘í•œ NLP taskì— ì‚¬ìš© ê°€ëŠ¥

> âœ… ì„±ëŠ¥ ë§¤ìš° ìš°ìˆ˜  
> âŒ GPU ìì› í•„ìš”


## ğŸ§  5. í•œêµ­ì–´ ë¶„ë¥˜ ëª¨ë¸ (KcELECTRA ë“±)

```python
from transformers import AutoTokenizer, AutoModelForSequenceClassification, pipeline

model_name = "beomi/KcELECTRA-base"
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=3)

text_classifier = pipeline("text-classification", model=model, tokenizer=tokenizer)
text = "ì§„ì§œ ì´ ì˜ìƒ ë„ˆë¬´ ì›ƒê²¨ìš” ã…‹ã…‹"
print(text_classifier(text))
```

> Hugging Face ëª¨ë¸ í—ˆë¸Œì—ì„œ ë‹¤ì–‘í•œ í•œêµ­ì–´ ëª¨ë¸ ì‚¬ìš© ê°€ëŠ¥


## ğŸ“š ì •ë¦¬í‘œ

| ì„¸ëŒ€ | ê¸°ìˆ  | ì¥ì  | ë‹¨ì  |
|------|------|------|------|
| 1ì„¸ëŒ€ | BoW, TF-IDF | ë‹¨ìˆœí•˜ê³  ë¹ ë¦„ | ì˜ë¯¸ ë°˜ì˜ ë¶ˆê°€ |
| 2ì„¸ëŒ€ | Word2Vec | ì˜ë¯¸ ë°˜ì˜ | ë¬¸ì¥ í‘œí˜„ ì–´ë ¤ì›€ |
| 3ì„¸ëŒ€ | LSTM | ë¬¸ë§¥ ì²˜ë¦¬ | ìˆœì°¨ ì²˜ë¦¬ ëŠë¦¼ |
| 4ì„¸ëŒ€ | BERT | ìµœê³  ì •í™•ë„ | GPU ìì› í•„ìš” |


ë¬¼ë¡ ì…ë‹ˆë‹¤! ì•„ë˜ëŠ” ê° ê¸°ìˆ ë³„ë¡œ ê°„ë‹¨í•œ **ì˜ˆì‹œ ì½”ë“œ**ë§Œ ë”°ë¡œ ì •ë¦¬í•œ ê²ƒì…ë‹ˆë‹¤.  

---

## ğŸ§© 1. BoW / TF-IDF ì˜ˆì‹œ (scikit-learn)

```python
from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer

texts = ["ì˜¤ëŠ˜ ë‚ ì”¨ê°€ ì •ë§ ì¢‹ë‹¤", "ì˜¤ëŠ˜ì€ ê¸°ë¶„ì´ ë„ˆë¬´ ì¢‹ì•„", "ë¹„ê°€ ì™€ì„œ ìš°ìš¸í•´"]

# BoW
bow_vectorizer = CountVectorizer()
bow = bow_vectorizer.fit_transform(texts)
print("BoW:\n", bow.toarray())

# TF-IDF
tfidf_vectorizer = TfidfVectorizer()
tfidf = tfidf_vectorizer.fit_transform(texts)
print("TF-IDF:\n", tfidf.toarray())
```

---

## ğŸ§¬ 2. Word2Vec ì„ë² ë”© ì˜ˆì‹œ (gensim)

```python
from gensim.models import Word2Vec

sentences = [["ì˜¤ëŠ˜", "ë‚ ì”¨", "ì¢‹ë‹¤"], ["ê¸°ë¶„", "ì¢‹ì•„"], ["ë¹„", "ìš°ìš¸"]]
model = Word2Vec(sentences, vector_size=100, window=2, min_count=1, sg=0)  # CBOW

print("ê¸°ë¶„ ë²¡í„°:\n", model.wv["ê¸°ë¶„"])
print("ìœ ì‚¬ë„(ê¸°ë¶„, ì¢‹ì•„):", model.wv.similarity("ê¸°ë¶„", "ì¢‹ì•„"))
```

---

## ğŸ§  3. LSTM ë¶„ë¥˜ê¸° ì˜ˆì‹œ (PyTorch)

```python
import torch.nn as nn

class SimpleLSTMClassifier(nn.Module):
    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim):
        super().__init__()
        self.embedding = nn.Embedding(vocab_size, embedding_dim)
        self.lstm = nn.LSTM(embedding_dim, hidden_dim, batch_first=True)
        self.fc = nn.Linear(hidden_dim, output_dim)

    def forward(self, x):
        embedded = self.embedding(x)
        _, (hidden, _) = self.lstm(embedded)
        return self.fc(hidden[-1])
```

> â€» í•™ìŠµìš© ë°ì´í„°ì™€ í† í¬ë‚˜ì´ì§•ì€ ìƒëµ

---

## ğŸ”® 4. BERT ë¶„ë¥˜ ì˜ˆì‹œ (transformers)

```python
from transformers import BertTokenizer, BertForSequenceClassification
from transformers import pipeline

model_name = "bert-base-multilingual-cased"
tokenizer = BertTokenizer.from_pretrained(model_name)
model = BertForSequenceClassification.from_pretrained(model_name, num_labels=2)

classifier = pipeline("text-classification", model=model, tokenizer=tokenizer)
print(classifier("ì˜¤ëŠ˜ ì •ë§ ê¸°ë¶„ì´ ì¢‹ë‹¤!"))
```

---

## ğŸ§  5. í•œêµ­ì–´ ë¶„ë¥˜ ëª¨ë¸ ì˜ˆì‹œ (KcELECTRA)

```python
from transformers import AutoTokenizer, AutoModelForSequenceClassification, pipeline

model_name = "beomi/KcELECTRA-base"
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=3)

text_classifier = pipeline("text-classification", model=model, tokenizer=tokenizer)
print(text_classifier("ì§„ì§œ ì´ ì˜ìƒ ë„ˆë¬´ ì›ƒê²¨ìš” ã…‹ã…‹"))
```

---




