
# 🧠 Q-Learning 이론 자세히 설명

## 📌 1. Q-Learning이란?

**Q-Learning**은 강화학습에서 가장 널리 쓰이는 **오프폴리시(Off-policy)** 학습 알고리즘입니다.

> 에이전트가 상태(state)에서 행동(action)을 취했을 때 기대되는 누적 보상(total reward)을 학습합니다.

Q-Learning은 **Q함수** (state-action value function)를 테이블 형식으로 직접 추정하여 최적의 정책을 찾아갑니다.

---

## 🔎 2. Q 함수란?

- Q(s, a): 상태 `s`에서 행동 `a`를 했을 때 기대되는 **미래 누적 보상**의 합 (expected return)
- `Q: S × A → ℝ` 로 정의되는 함수

> 이 값을 반복적으로 갱신하며, 가장 큰 Q 값을 갖는 행동이 최선의 행동으로 간주됩니다.

---

## 🧮 3. Bellman Equation

Q값은 **Bellman 방정식**을 바탕으로 정의됩니다:

```
Q(s, a) = E[r + γ * max Q(s', a') | s, a]
```

- `r`: 현재 상태에서 행동 `a`를 했을 때 받은 **즉시 보상**
- `γ`: 할인율 (0 ≤ γ ≤ 1), 미래 보상에 대한 신뢰도
- `s'`: 다음 상태
- `a'`: 다음 상태에서 가능한 모든 행동 중 최선

---

## 🔁 4. Q-Learning 업데이트 수식

학습 과정에서 Q 테이블은 아래와 같이 갱신됩니다:

```
Q(s, a) ← Q(s, a) + α * [r + γ * max Q(s', a') - Q(s, a)]
```

- `α`: 학습률 (learning rate), 얼마나 빠르게 업데이트할지 결정
- `r + γ * max Q(s', a')`: 목표값(target)
- `Q(s, a)`: 예측값(prediction)

> 이는 **Temporal Difference (TD) 학습**의 한 형태로, 실제 관측된 보상과 예측된 보상의 차이를 기반으로 업데이트합니다.

---

## 🎯 5. 정책 선택: ε-greedy

Q-Learning은 탐험과 이용의 균형을 위해 **ε-greedy** 정책을 사용합니다.

- 확률 ε만큼 **무작위 행동**
- 확률 1-ε만큼 **Q 값이 가장 높은 행동**

> 처음에는 ε가 크고, 점차 줄이며 최적 정책으로 수렴합니다.

---

## 📊 6. 학습 알고리즘 요약 (Pseudocode)

```python
초기화: Q 테이블을 0으로 초기화
for 에피소드 in 전체 에피소드 수:
    상태 s 초기화
    while 종료 상태가 아닐 때:
        ε 확률로 랜덤 행동 a 선택, 아니면 max Q(s,a)
        행동 a 실행 → 보상 r, 다음 상태 s'
        Q(s,a) ← Q(s,a) + α * [r + γ * max Q(s',a') - Q(s,a)]
        상태 업데이트: s ← s'
```

---

## 🌍 7. 간단한 예시: 미로

```
S - 시작지점
G - 목표지점
H - 구멍(빠지면 종료)
F - 안전한 지점

[S, F, H, G]
```

- Q 테이블은 상태-행동 쌍에 대한 2차원 배열입니다.
- 반복적으로 환경을 탐험하며 `Q(s,a)`를 업데이트
- 최종적으로는 `G`로 가는 최적 경로를 알게 됨

---

## 🧠 8. Q-Learning의 특성

| 항목 | 설명 |
|------|------|
| 종류 | Off-policy (목표 정책과 행동 정책이 다름) |
| 탐험 방식 | ε-greedy |
| 상태공간 | 작을 경우에만 테이블 방식으로 가능 |
| 수렴 보장 | 적절한 ε 감소 및 α 조건 만족 시 수렴함 (보장된 수학적 근거 존재) |

---

## 🧱 9. 한계점

- 상태/행동 공간이 커지면 **Q 테이블 크기 폭발**
- 이미지 입력 등 고차원에서는 사용 불가 → 딥러닝 기반 Q 함수 근사(DQN) 필요

---

## 📚 참고 문헌

- Sutton, R. S., & Barto, A. G. (2018). *Reinforcement Learning: An Introduction*.
- Watkins & Dayan (1992). *Q-Learning* 원 논문.

---

## 📘 다음 단계

- Q-Learning을 딥러닝으로 확장한 **DQN**
- 정책 기반 방법 (Policy Gradient)
- Actor-Critic 계열 (A2C, A3C, PPO)
